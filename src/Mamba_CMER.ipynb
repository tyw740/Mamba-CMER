{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 数据导入\n"
      ],
      "metadata": {
        "id": "O4NzbcUUgH4Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLmNfZD9RZLI"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "!pip list\n",
        "!uv pip install mamba-ssm --no-build-isolation -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-fJIM3BL_C6"
      },
      "outputs": [],
      "source": [
        "# 使用gdown下载数据\n",
        "import gdown\n",
        "\n",
        "file_id = '1SwOr5V-eUaES_aWfdO6C1ykAuja08ipf'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'Raw.zip', quiet=False)\n",
        "\n",
        "file_id = '1cPwMp98Wwew-YIaFjD7lWUvg9-V9f_WK'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'MOSI-label.csv', quiet=False)\n",
        "\n",
        "file_id = '1MFzHi-g3wNzQ3dbDMr_0wtAkrcPIaVvI'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'aligned_50.pkl', quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/柒肆零/*.npy ./"
      ],
      "metadata": {
        "id": "LFQQJ2_N1dnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvS0_itQ9pVP"
      },
      "outputs": [],
      "source": [
        "!unzip /content/Raw.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfYoDJT3Mg9F"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fusIFVWmMkYL"
      },
      "outputs": [],
      "source": [
        "pickle_filename = '/content/aligned_50.pkl'\n",
        "csv_filename = '/content/MOSI-label.csv'\n",
        "n_class = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 提取图像文本语音特征"
      ],
      "metadata": {
        "id": "z9imO9Fkjc1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, CLIPTextModel\n",
        "\n",
        "def extract_clip_token_embeddings(model, tokenizer, texts, max_length=20, device=\"cpu\"):\n",
        "\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "\n",
        "    # 编码\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    ).to(device)\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        hidden_states = outputs.last_hidden_state  # shape: (B, seq_len, 512)\n",
        "\n",
        "    return hidden_states[0].cpu().numpy()\n",
        "from transformers import AutoTokenizer, CLIPTextModel\n",
        "\n",
        "# 加载模型\n",
        "t_clip_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "t_clip_tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "texts = \"a photo of a cat\"\n",
        "hidden_states = extract_clip_token_embeddings(t_clip_model, t_clip_tokenizer, texts, device=device)\n",
        "\n",
        "print(\"Token embeddings shape:\", hidden_states.shape)\n"
      ],
      "metadata": {
        "id": "cfoAzSNTvWPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import AutoProcessor, CLIPVisionModelWithProjection\n",
        "\n",
        "img_clip_model = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "img_clip_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "inputs = img_clip_processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "outputs = img_clip_model(**inputs)\n",
        "image_embeds = outputs.image_embeds"
      ],
      "metadata": {
        "id": "jqVoe-Q_-g5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import CLIPVisionModelWithProjection, AutoProcessor\n",
        "\n",
        "def extract_clip_features_from_video(model, processor, video_path, num_frames=5, device=\"cpu\"):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    if frame_count == 0:\n",
        "        cap.release()\n",
        "        raise ValueError(\"无法读取视频或视频为空\")\n",
        "\n",
        "    frame_indices = np.linspace(0, frame_count - 1, num_frames, dtype=int)\n",
        "    embeddings = []\n",
        "\n",
        "    for idx in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            print(f\"警告：无法读取第 {idx} 帧，跳过\")\n",
        "            continue\n",
        "\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        pil_image = Image.fromarray(frame_rgb)\n",
        "\n",
        "        # 图像预处理和特征提取\n",
        "        inputs = processor(images=pil_image, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embedding = outputs.image_embeds.squeeze(0).cpu().numpy()\n",
        "        embeddings.append(embedding)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(embeddings) == 0:\n",
        "        raise ValueError(\"未成功提取任何帧的特征\")\n",
        "\n",
        "    return np.stack(embeddings)\n",
        "\n",
        "\n",
        "clip_model = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "v_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "video_file ='/content/Video/Segmented/03bSnISJMiM_10.mp4'\n",
        "features = extract_clip_features_from_video(clip_model, v_processor, video_file, num_frames=5, device=device)\n",
        "\n",
        "print(\"提取特征 shape:\", features.shape)\n"
      ],
      "metadata": {
        "id": "E_fC6beTrghW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 提取音频特征"
      ],
      "metadata": {
        "id": "FjaUyQ7VrhHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "def extract_fixed_mfcc(wav_path, sr=16000, duration=1.0, n_mfcc=13, n_fft=400, hop_length=160):\n",
        "\n",
        "    target_length = int(sr * duration)\n",
        "\n",
        "    # 加载音频\n",
        "    y, _ = librosa.load(wav_path, sr=sr)\n",
        "\n",
        "    if len(y) < target_length:\n",
        "        y = np.pad(y, (0, target_length - len(y)))\n",
        "    else:\n",
        "        y = y[:target_length]\n",
        "\n",
        "    # 提取 MFCC 特征\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
        "\n",
        "    return mfcc[:,:100]\n"
      ],
      "metadata": {
        "id": "iJKvuj_orgn3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 合并处理文本 图像 音频"
      ],
      "metadata": {
        "id": "_tMGtCqttbpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " import tqdm\n",
        "\n",
        " video_dir = '/content/Video/Segmented/'\n",
        " audio_dir = '/content/Audio/WAV_16000/Segmented/'\n",
        " vision_feats = []\n",
        " audio_feats = []\n",
        " text_feats = []\n",
        " for idx,row in tqdm.tqdm(pd_data.iterrows(),total = len(pd_data)):\n",
        "   video_file = video_dir+row['video_file']\n",
        "   audio_file = audio_dir+row['audio_file']\n",
        "   one_text = row['text']\n",
        "   one_vision_feat = extract_clip_features_from_video(clip_model, v_processor, video_file,5,device)\n",
        "   one_audio_feat = extract_fixed_mfcc(audio_file)\n",
        "   one_text_feat = extract_clip_token_embeddings(t_clip_model, t_clip_tokenizer, one_text, device=device)\n",
        "   vision_feats.append(one_vision_feat)\n",
        "   audio_feats.append(one_audio_feat)\n",
        "   text_feats.append(one_text_feat)\n",
        "    break\n",
        " # 转换为 numpy 数组\n",
        " vision_feats = np.stack(vision_feats)\n",
        " audio_feats = np.stack(audio_feats)\n",
        " text_feats = np.stack(text_feats)\n",
        "\n",
        " print(\"视觉特征 shape:\", vision_feats.shape)\n",
        " print(\"音频特征 shape:\", audio_feats.shape)\n",
        " print(\"文本特征 shape:\", text_feats.shape)\n",
        "\n",
        " np.save(\"vision_feats.npy\", vision_feats)\n",
        " np.save(\"audio_feats.npy\", audio_feats)\n",
        " np.save(\"text_feats.npy\", text_feats)\n",
        "\n",
        " # 加载npy文件为变量\n",
        " vision_feats = np.load(\"vision_feats.npy\")\n",
        " audio_feats = np.load(\"audio_feats.npy\")\n",
        " text_feats = np.load(\"text_feats.npy\")\n",
        " print(\"视觉特征 shape:\", vision_feats.shape)\n",
        " print(\"音频特征 shape:\", audio_feats.shape)\n",
        " print(\"文本特征 shape:\", text_feats.shape)"
      ],
      "metadata": {
        "id": "IMc7EbPAtfts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXKhXz76jFZq"
      },
      "source": [
        "### 数据处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNkWtFImjUrC"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, vision_feats,audio_feats,text_feats,pd_data):\n",
        "      self.vision_data = vision_feats\n",
        "      self.audio_data = audio_feats\n",
        "      self.text_data = text_feats\n",
        "      self.labels = pd_data['label_num'].values\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.vision_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # 图像数据处理\n",
        "        one_img = self.vision_data[idx]\n",
        "        one_img = torch.tensor(one_img).float()\n",
        "\n",
        "        # 文本数据处理\n",
        "        one_text = self.text_data[idx]\n",
        "        one_text = torch.tensor(one_text).float()\n",
        "\n",
        "        # 语音数据处理\n",
        "        one_audio = self.audio_data[idx]\n",
        "        one_audio = torch.tensor(one_audio).float()\n",
        "\n",
        "\n",
        "        # label数据处理\n",
        "        one_label = self.labels[idx]\n",
        "        one_label = torch.tensor(one_label).long()\n",
        "        return one_img, one_text, one_audio, one_label\n",
        "\n",
        "\n",
        "all_ds = CustomDataset(vision_feats,audio_feats,text_feats,pd_data)\n",
        "train_ds, test_ds = torch.utils.data.random_split(all_ds, [0.8, 0.2])\n",
        "train_ds[1][0].shape,train_ds[1][1].shape,train_ds[1][2].shape,len(train_ds),len(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds_1, test_ds_2 = torch.utils.data.random_split(test_ds, [0.3, 0.7])\n",
        "len(test_ds_1),len(test_ds_2)\n",
        "\n",
        "# 合并数据集\n",
        "train_ds = torch.utils.data.ConcatDataset([train_ds, test_ds_2])\n",
        "len(train_ds)\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
        "test_dl = DataLoader(test_ds, batch_size=8)"
      ],
      "metadata": {
        "id": "7w451I3jDcFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lU7H9hM3zXb"
      },
      "source": [
        "### 模型创新"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90oH-rdWx79P"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, mamba_inner_fn\n",
        "\n",
        "try:\n",
        "    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n",
        "except ImportError:\n",
        "    causal_conv1d_fn, causal_conv1d_update = None, None\n",
        "\n",
        "try:\n",
        "    from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n",
        "except ImportError:\n",
        "    selective_state_update = None\n",
        "\n",
        "try:\n",
        "    from mamba_ssm.ops.triton.layer_norm import RMSNorm, layer_norm_fn, rms_norm_fn\n",
        "except ImportError:\n",
        "    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None\n",
        "\n",
        "\n",
        "class Mamba(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model,\n",
        "        d_state=16,\n",
        "        d_conv=4,\n",
        "        expand=2,\n",
        "        dt_rank=\"auto\",\n",
        "        dt_min=0.001,\n",
        "        dt_max=0.1,\n",
        "        dt_init=\"random\",\n",
        "        dt_scale=1.0,\n",
        "        dt_init_floor=1e-4,\n",
        "        conv_bias=True,\n",
        "        bias=False,\n",
        "        use_fast_path=True,\n",
        "        layer_idx=None,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "    ):\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_state = d_state\n",
        "        self.d_conv = d_conv\n",
        "        self.expand = expand\n",
        "        self.d_inner = int(self.expand * self.d_model)\n",
        "        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank\n",
        "        self.use_fast_path = use_fast_path\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs)\n",
        "\n",
        "        self.conv1d = nn.Conv1d(\n",
        "            in_channels=self.d_inner,\n",
        "            out_channels=self.d_inner,\n",
        "            bias=conv_bias,\n",
        "            kernel_size=d_conv,\n",
        "            groups=self.d_inner,\n",
        "            padding=d_conv - 1,\n",
        "            **factory_kwargs,\n",
        "        )\n",
        "\n",
        "        self.activation = \"silu\"\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        self.x_proj = nn.Linear(\n",
        "            self.d_inner, self.dt_rank + self.d_state * 2, bias=False, **factory_kwargs\n",
        "        )\n",
        "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True, **factory_kwargs)\n",
        "\n",
        "        dt_init_std = self.dt_rank**-0.5 * dt_scale\n",
        "        if dt_init == \"constant\":\n",
        "            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n",
        "        elif dt_init == \"random\":\n",
        "            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        dt = torch.exp(\n",
        "            torch.rand(self.d_inner, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))\n",
        "            + math.log(dt_min)\n",
        "        ).clamp(min=dt_init_floor)\n",
        "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
        "        with torch.no_grad():\n",
        "            self.dt_proj.bias.copy_(inv_dt)\n",
        "        self.dt_proj.bias._no_reinit = True\n",
        "\n",
        "        A = repeat(\n",
        "            torch.arange(1, self.d_state + 1, dtype=torch.float32, device=device),\n",
        "            \"n -> d n\",\n",
        "            d=self.d_inner,\n",
        "        ).contiguous()\n",
        "        A_log = torch.log(A)\n",
        "        self.A_log = nn.Parameter(A_log)\n",
        "        self.A_log._no_weight_decay = True\n",
        "        self.D = nn.Parameter(torch.ones(self.d_inner, device=device))\n",
        "        self.D._no_weight_decay = True\n",
        "\n",
        "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)\n",
        "\n",
        "    def forward(self, hidden_states, inference_params=None):\n",
        "\n",
        "        batch, seqlen, dim = hidden_states.shape\n",
        "\n",
        "        conv_state, ssm_state = None, None\n",
        "        if inference_params is not None:\n",
        "            conv_state, ssm_state = self._get_states_from_cache(inference_params, batch)\n",
        "            if inference_params.seqlen_offset > 0:\n",
        "                out, _, _ = self.step(hidden_states, conv_state, ssm_state)\n",
        "                return out\n",
        "        xz = rearrange(\n",
        "            self.in_proj.weight @ rearrange(hidden_states, \"b l d -> d (b l)\"),\n",
        "            \"d (b l) -> b d l\",\n",
        "            l=seqlen,\n",
        "        )\n",
        "        if self.in_proj.bias is not None:\n",
        "            xz = xz + rearrange(self.in_proj.bias.to(dtype=xz.dtype), \"d -> d 1\")\n",
        "\n",
        "        A = -torch.exp(self.A_log.float())\n",
        "        if self.use_fast_path and causal_conv1d_fn is not None and inference_params is None:\n",
        "            out = mamba_inner_fn(\n",
        "                xz,\n",
        "                self.conv1d.weight,\n",
        "                self.conv1d.bias,\n",
        "                self.x_proj.weight,\n",
        "                self.dt_proj.weight,\n",
        "                self.out_proj.weight,\n",
        "                self.out_proj.bias,\n",
        "                A,\n",
        "                None,\n",
        "                None,\n",
        "                self.D.float(),\n",
        "                delta_bias=self.dt_proj.bias.float(),\n",
        "                delta_softplus=True,\n",
        "            )\n",
        "        else:\n",
        "            x, z = xz.chunk(2, dim=1)\n",
        "            if conv_state is not None:\n",
        "                conv_state.copy_(F.pad(x, (self.d_conv - x.shape[-1], 0)))\n",
        "            if causal_conv1d_fn is None:\n",
        "                x = self.act(self.conv1d(x)[..., :seqlen])\n",
        "            else:\n",
        "                assert self.activation in [\"silu\", \"swish\"]\n",
        "                x = causal_conv1d_fn(\n",
        "                    x=x,\n",
        "                    weight=rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
        "                    bias=self.conv1d.bias,\n",
        "                    activation=self.activation,\n",
        "                )\n",
        "            x_dbl = self.x_proj(rearrange(x, \"b d l -> (b l) d\"))\n",
        "            dt, B, C = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n",
        "            dt = self.dt_proj.weight @ dt.t()\n",
        "            dt = rearrange(dt, \"d (b l) -> b d l\", l=seqlen)\n",
        "            B = rearrange(B, \"(b l) dstate -> b dstate l\", l=seqlen).contiguous()\n",
        "            C = rearrange(C, \"(b l) dstate -> b dstate l\", l=seqlen).contiguous()\n",
        "            assert self.activation in [\"silu\", \"swish\"]\n",
        "            y = selective_scan_fn(\n",
        "                x,\n",
        "                dt,\n",
        "                A,\n",
        "                B,\n",
        "                C,\n",
        "                self.D.float(),\n",
        "                z=z,\n",
        "                delta_bias=self.dt_proj.bias.float(),\n",
        "                delta_softplus=True,\n",
        "                return_last_state=ssm_state is not None,\n",
        "            )\n",
        "            if ssm_state is not None:\n",
        "                y, last_state = y\n",
        "                ssm_state.copy_(last_state)\n",
        "            y = rearrange(y, \"b d l -> b l d\")\n",
        "            out = self.out_proj(y)\n",
        "        return out\n",
        "\n",
        "    def step(self, hidden_states, conv_state, ssm_state):\n",
        "        dtype = hidden_states.dtype\n",
        "        assert hidden_states.shape[1] == 1, \"Only support decoding with 1 token at a time for now\"\n",
        "        xz = self.in_proj(hidden_states.squeeze(1))\n",
        "        x, z = xz.chunk(2, dim=-1)\n",
        "\n",
        "        if causal_conv1d_update is None:\n",
        "            conv_state.copy_(torch.roll(conv_state, shifts=-1, dims=-1))\n",
        "            conv_state[:, :, -1] = x\n",
        "            x = torch.sum(conv_state * rearrange(self.conv1d.weight, \"d 1 w -> d w\"), dim=-1)\n",
        "            if self.conv1d.bias is not None:\n",
        "                x = x + self.conv1d.bias\n",
        "            x = self.act(x).to(dtype=dtype)\n",
        "        else:\n",
        "            x = causal_conv1d_update(\n",
        "                x,\n",
        "                conv_state,\n",
        "                rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
        "                self.conv1d.bias,\n",
        "                self.activation,\n",
        "            )\n",
        "\n",
        "        x_db = self.x_proj(x)\n",
        "        dt, B, C = torch.split(x_db, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n",
        "        dt = F.linear(dt, self.dt_proj.weight)\n",
        "        A = -torch.exp(self.A_log.float())\n",
        "\n",
        "        if selective_state_update is None:\n",
        "\n",
        "            dt = F.softplus(dt + self.dt_proj.bias.to(dtype=dt.dtype))\n",
        "            dA = torch.exp(torch.einsum(\"bd,dn->bdn\", dt, A))\n",
        "            dB = torch.einsum(\"bd,bn->bdn\", dt, B)\n",
        "            ssm_state.copy_(ssm_state * dA + rearrange(x, \"b d -> b d 1\") * dB)\n",
        "            y = torch.einsum(\"bdn,bn->bd\", ssm_state.to(dtype), C)\n",
        "            y = y + self.D.to(dtype) * x\n",
        "            y = y * self.act(z)  # (B D)\n",
        "        else:\n",
        "            y = selective_state_update(\n",
        "                ssm_state, x, dt, A, B, C, self.D, z=z, dt_bias=self.dt_proj.bias, dt_softplus=True\n",
        "            )\n",
        "\n",
        "        out = self.out_proj(y)\n",
        "        return out.unsqueeze(1), conv_state, ssm_state\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        device = self.out_proj.weight.device\n",
        "        conv_dtype = self.conv1d.weight.dtype if dtype is None else dtype\n",
        "        conv_state = torch.zeros(\n",
        "            batch_size, self.d_model * self.expand, self.d_conv, device=device, dtype=conv_dtype\n",
        "        )\n",
        "        ssm_dtype = self.dt_proj.weight.dtype if dtype is None else dtype\n",
        "\n",
        "        ssm_state = torch.zeros(\n",
        "            batch_size, self.d_model * self.expand, self.d_state, device=device, dtype=ssm_dtype\n",
        "        )\n",
        "        return conv_state, ssm_state\n",
        "\n",
        "    def _get_states_from_cache(self, inference_params, batch_size, initialize_states=False):\n",
        "        assert self.layer_idx is not None\n",
        "        if self.layer_idx not in inference_params.key_value_memory_dict:\n",
        "            batch_shape = (batch_size,)\n",
        "            conv_state = torch.zeros(\n",
        "                batch_size,\n",
        "                self.d_model * self.expand,\n",
        "                self.d_conv,\n",
        "                device=self.conv1d.weight.device,\n",
        "                dtype=self.conv1d.weight.dtype,\n",
        "            )\n",
        "            ssm_state = torch.zeros(\n",
        "                batch_size,\n",
        "                self.d_model * self.expand,\n",
        "                self.d_state,\n",
        "                device=self.dt_proj.weight.device,\n",
        "                dtype=self.dt_proj.weight.dtype,\n",
        "\n",
        "            )\n",
        "            inference_params.key_value_memory_dict[self.layer_idx] = (conv_state, ssm_state)\n",
        "        else:\n",
        "            conv_state, ssm_state = inference_params.key_value_memory_dict[self.layer_idx]\n",
        "\n",
        "            if initialize_states:\n",
        "                conv_state.zero_()\n",
        "                ssm_state.zero_()\n",
        "        return conv_state, ssm_state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg6yRZa932VY"
      },
      "source": [
        "### 定义mamba模块参数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNBMWHkyx76J"
      },
      "outputs": [],
      "source": [
        "\n",
        "batch, length, dim = 2, 3, 100\n",
        "x = torch.randn(batch, length, dim).to(\"cuda\")\n",
        "model = Mamba(\n",
        "    d_model=dim,\n",
        "    d_state=16,\n",
        "    d_conv=4,\n",
        "    expand=2,\n",
        ").to(\"cuda\")\n",
        "y = model(x)\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvyiOBA0csSD"
      },
      "source": [
        "### mamba注意力机制"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yo1QN6j9cuEi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class MambaAttention(nn.Module):\n",
        "    def __init__(self, d_model, dim_feedforward=2048, dropout=0.1):\n",
        "        super(MambaAttention, self).__init__()\n",
        "        self.self_attn = Mamba(d_model=d_model, d_state=16, d_conv=4,expand=2)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # Self Attention\n",
        "        src2 = self.self_attn(src)\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "\n",
        "        # Feedforward Network\n",
        "        src2 = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "mamba_att = MambaAttention(100).to(device)\n",
        "\n",
        "src = torch.rand(10, 3, 100).to(device)\n",
        "output = mamba_att(src)\n",
        "\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukVEayhFuiVX"
      },
      "source": [
        "### 整体模型构建"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTp21ad5uiVX"
      },
      "outputs": [],
      "source": [
        "class bigModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.img_block = img_block\n",
        "        self.text_block = text_block\n",
        "        self.audio_block = audio_block\n",
        "        self.relu = nn.ReLU()\n",
        "        self.lin1 = nn.Linear(600, 50)\n",
        "        self.lin2 = nn.Linear(50, n_class)\n",
        "        # self.att = CBAM1D(in_channels=3, reduction=4, spatial_kernel=7)\n",
        "        self.att = MambaAttention(d_model=100)\n",
        "        self.cnn = nn.Conv1d(6, 6, 3,padding='same')\n",
        "        self.ca1 = nn.MultiheadAttention(100,2)\n",
        "        self.ca2 = nn.MultiheadAttention(100,2)\n",
        "        self.ca3 = nn.MultiheadAttention(100,2)\n",
        "\n",
        "\n",
        "    def forward(self, img,text,audio):\n",
        "        x1 = self.img_block(img)\n",
        "        x2 = self.text_block(text)\n",
        "        x3 = self.audio_block(audio)\n",
        "        # x = torch.stack([x1,x2,x3],dim=1)\n",
        "        x4,_ = self.ca1(x1,x2,x2)\n",
        "        x5,_ = self.ca2(x2,x3,x3)\n",
        "        x6,_ = self.ca3(x3,x1,x1)\n",
        "        x = torch.stack([x1,x2,x3,x4,x5,x6],dim=1)\n",
        "        # print(x.shape)\n",
        "        x = self.att(x)\n",
        "        x = self.cnn(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.lin1(x)\n",
        "        x = self.lin2(x)\n",
        "        # x =\n",
        "        return x,x1,x2,x3\n",
        "big_model = bigModel().to(device)\n",
        "big_model(b_img.to(device),b_text.to(device),b_audio.to(device))[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOPANXFRuiVX"
      },
      "source": [
        "### 损失函数和优化器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyU8TzQxuiVX"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(big_model.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 对比学习函数"
      ],
      "metadata": {
        "id": "7UBxX_jjqKyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CtrastiveLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.07):\n",
        "\n",
        "        super(CtrastiveLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.cosine_similarity = nn.CosineSimilarity(dim=-1)\n",
        "\n",
        "    def forward(self, features_i, features_j):\n",
        "\n",
        "        features_i = F.normalize(features_i, dim=1)\n",
        "        features_j = F.normalize(features_j, dim=1)\n",
        "\n",
        "        similarity_matrix = torch.matmul(features_i, features_j.T)  # Shape: [N, N]\n",
        "\n",
        "\n",
        "        similarity_matrix = similarity_matrix / self.temperature\n",
        "\n",
        "\n",
        "        labels = torch.arange(features_i.size(0)).to(features_i.device)\n",
        "\n",
        "        loss_i_to_j = F.cross_entropy(similarity_matrix, labels)\n",
        "        loss_j_to_i = F.cross_entropy(similarity_matrix.T, labels)\n",
        "\n",
        "        loss = (loss_i_to_j + loss_j_to_i) / 2\n",
        "        return loss\n",
        "\n",
        "ctrastive_loss = CtrastiveLoss()\n",
        "fea1 = torch.randn(10, 100)\n",
        "fea2 = torch.randn(10, 100)\n",
        "ctrastive_loss(fea1, fea2)"
      ],
      "metadata": {
        "id": "NOoixJfnp017"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW3ftgJ0uiVY"
      },
      "source": [
        "### 模型训练和测试"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KX6x0VxuiVY"
      },
      "outputs": [],
      "source": [
        "best_acc = 0.0\n",
        "best_model_path = 'best_model_weights.pth'\n",
        "epochs = 20\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_loss_list = []\n",
        "test_acc_list = []\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_cl(train_dl, big_model, loss_fn, optimizer)\n",
        "\n",
        "    train_loss, train_correct = test_cl(train_dl, big_model, loss_fn)\n",
        "    test_loss, test_correct = test_cl(test_dl, big_model, loss_fn)\n",
        "\n",
        "    train_loss_list.append(train_loss)\n",
        "    train_acc_list.append(train_correct)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_correct)\n",
        "\n",
        "    if test_correct > best_acc:\n",
        "        best_acc = test_correct\n",
        "        torch.save(big_model.state_dict(), best_model_path)\n",
        "        print(f\"New best accuracy of {best_acc}% achieved at epoch {t+1}! Model weights saved.\")\n",
        "\n",
        "big_model.load_state_dict(torch.load(best_model_path))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
